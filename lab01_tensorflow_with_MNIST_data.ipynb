{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Official MNIST Example from TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # Import data\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "    # Define regression model. At this point, it's linear regression.\n",
    "    # You may call this the computational `Graph`.\n",
    "    # placeholder is a box we can put data into\n",
    "    # variable is something TensorFlow must change during optimization\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    y = tf.matmul(x, W) + b\n",
    "\n",
    "    # Places where we store labels\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Define cross entropy function\n",
    "    # tf.nn is Tensorflow neural network support module\n",
    "    # reduce_mean is a built-in math operators, which computes means of a tensor\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    \n",
    "    # We set up a Gradient Descent Optimizer. \n",
    "    # This is an operation that we can execute later.\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "    \n",
    "    # We have defined our tensors and operations. \n",
    "    # We need to create session to evaluate those variables and ops.\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    # Here we call global variables initializer ops, and run it.\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # Training\n",
    "    # In each iteration, we call training data in batch\n",
    "    # We ask session to run ops `train_step` with `batch_xs` in the `x` PlaceHolder\n",
    "    # and `batch_ys` in the `y_` PlaceHolder\n",
    "    for _ in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    # Testing\n",
    "    # Now we define a new operation in our graph, called `correct_prediction`,\n",
    "    # which compares between actual labels and predictions.\n",
    "    # tf.argmax gives you the label of the highest value in the tensor\n",
    "    # Then, we define another operation called `accuracy` to compute average percent accuracy.\n",
    "    # We let session run graph, with our test data.\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                      y_: mnist.test.labels}))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # FLAGS is Google's argparse interface\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    \n",
    "    # this make sure that the main function is run on command line \n",
    "    # with cmd arguments from parser\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Let's try something fun.\n",
    "\n",
    "1. Add a piece of code to compute the accuracy of our model when used with the training data, to see if our datasets are overfitted or not.\n",
    "2. If we train the network for 5000 or 10000 iterations, instead of 1000, what happen? Does the network perform better? Is the network more overfitted?\n",
    "3. If we change the optimizer to AdagradOptimizer, what happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
